diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 9e4d738..1958d3f 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -890,6 +890,7 @@ asmlinkage long sys_membarrier(int cmd, int flags);
 asmlinkage long sys_mlock2(unsigned long start, size_t len, int flags);
 
 #ifdef CONFIG_SCHED_RTGANG
+asmlinkage long sys_npplock(int lock);
 asmlinkage long sys_rtg_set_params(pid_t pid,
 			struct rtg_resource_info __user *info);
 #endif
diff --git a/include/uapi/asm-generic/unistd.h b/include/uapi/asm-generic/unistd.h
index a90c346..046d0a9 100644
--- a/include/uapi/asm-generic/unistd.h
+++ b/include/uapi/asm-generic/unistd.h
@@ -665,6 +665,9 @@ __SC_COMP(__NR_recvmmsg, sys_recvmmsg, compat_sys_recvmmsg)
 #ifdef CONFIG_SCHED_RTGANG
 #define __NR_rtg_set_params 245
 __SYSCALL(__NR_rtg_set_params, sys_rtg_set_params)
+
+#define __NR_npplock 246
+__SYSCALL(__NR_npplock, sys_npplock)
 #endif
 
 #define __NR_wait4 260
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 6e64991..41e1766 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -1208,10 +1208,28 @@ struct task_struct *pick_next_task_dl(struct rq *rq, struct task_struct *prev)
 
 #ifdef CONFIG_SCHED_RTGANG
 	if (sched_feat(RT_GANG_LOCK)) {
-		ret = rtg_try_acquire_lock(p);
+		ret = rtg_try_acquire_lock(p, prev);
 
 		if (ret == RTG_BLOCK)
 			return BLOCK_TASK;
+
+		if (ret == RTG_NPP_BLOCK) {
+			printk(KERN_WARNING "[RT-GANG] NPP lock has not been "
+				"tested with deadline tasks. Use at your own "
+				"risk!\n");
+
+			if (prev->sched_class == &dl_sched_class) {
+				/*
+				 * The 'next' task cannot preempt the 'prev'
+				 * task because of the NPP lock. Since the
+				 * 'prev' task belongs to the deadline class,
+				 * we must return it back to the scheduler so
+				 * that it may continue execution.
+				 */
+				p = prev;
+			} else
+				return BLOCK_TASK;
+		}
 	}
 #endif
 
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index 9f8077f..01ed691 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -1541,19 +1541,32 @@ pick_next_task_rt(struct rq *rq, struct task_struct *prev)
 	if (!rt_rq->rt_queued)
 		return NULL;
 
-	p = __peek_next_task_rt (rq);
+	p = __peek_next_task_rt(rq);
 
 #ifdef CONFIG_SCHED_RTGANG
 	if (sched_feat(RT_GANG_LOCK) && RTG_FIFO_CHECK(p)) {
-		ret = rtg_try_acquire_lock(p);
+		ret = rtg_try_acquire_lock(p, prev);
 
-		if (ret == RTG_BLOCK)
+		if (ret == RTG_BLOCK || (ret == RTG_NPP_BLOCK &&
+					prev->sched_class != &rt_sched_class))
 			return BLOCK_TASK;
+
+		if (ret == RTG_NPP_BLOCK &&
+				prev->sched_class == &rt_sched_class) {
+			/*
+			 * The 'next' task cannot preempt the 'prev' task
+			 * because of the NPP lock. Since the 'prev' task
+			 * belongs to the RT sched class, we must return it
+			 * back to the scheduler so that it may continue
+			 * execution.
+			 */
+			p = prev;
+		}
 	}
 #endif
 
-	put_prev_task (rq, prev);
-	p->se.exec_start = rq_clock_task (rq);
+	put_prev_task(rq, prev);
+	p->se.exec_start = rq_clock_task(rq);
 
 	/* The running task is never eligible for pushing */
 	dequeue_pushable_task(rq, p);
diff --git a/kernel/sched/rtgang.c b/kernel/sched/rtgang.c
index 5917263..ee724e4 100644
--- a/kernel/sched/rtgang.c
+++ b/kernel/sched/rtgang.c
@@ -12,6 +12,7 @@
 #include "sched.h"
 #include "rtgang.h"
 #include <linux/debugfs.h>
+#include <linux/syscalls.h>
 
 #ifdef CONFIG_SCHED_THROTTLE
 #include "rtg_throttle.h"
@@ -37,10 +38,10 @@ static inline void gang_lock_cpu(struct task_struct *thread)
 	int cpu = smp_processor_id();
 
 	cpumask_set_cpu(cpu, rtg_lock->locked_cores);
-	rtg_lock->gthreads [cpu] = thread;
+	rtg_lock->gthreads[cpu] = thread;
 	rtg_debug(RTG_LEVEL_SUBSTATE, "    rtg_lock_thread: comm=%s sched=%s "
 			"pid=%d tgid=%d rtgid=%d\n", thread->comm, PRINT_SCHED(thread),
-			thread->pid, thread->tgid, thread->rtgid);
+			thread->pid, thread->tgid, GET_RTG_INFO(thread)->gid);
 
 	return;
 }
@@ -85,11 +86,11 @@ static inline void do_gang_preemption(struct task_struct *next)
 
 		rtg_debug(RTG_LEVEL_SUBSTATE, "    rtg_preempt_thread: cpu=%d "
 				"comm=%s sched=%s pid=%d tgid=%d rtgid=%d\n", cpu,
-				rtg_lock->gthreads [cpu]->comm,
-				PRINT_SCHED(rtg_lock->gthreads [cpu]),
-				rtg_lock->gthreads [cpu]->pid,
-				rtg_lock->gthreads [cpu]->tgid,
-				rtg_lock->gthreads [cpu]->rtgid);
+				rtg_lock->gthreads[cpu]->comm,
+				PRINT_SCHED(rtg_lock->gthreads[cpu]),
+				rtg_lock->gthreads[cpu]->pid,
+				rtg_lock->gthreads[cpu]->tgid,
+				GET_RTG_INFO(rtg_lock->gthreads[cpu])->gid);
 		rtg_lock->gthreads [cpu] = NULL;
 	}
 
@@ -116,17 +117,30 @@ static inline void try_glock_release(struct task_struct *thread)
 	 * Migrated tasks can hold lock on multiple cores.
 	 */
 	for_each_cpu (cpu, rtg_lock->locked_cores) {
-		if (rtg_lock->gthreads [cpu] == thread) {
+		if (rtg_lock->gthreads[cpu] == thread) {
 			WARN_ON(!rt_prio(thread->prio));
+
+			rtg_lock->gthreads[cpu] = NULL;
 			cpumask_clear_cpu(cpu, rtg_lock->locked_cores);
+
 			rtg_debug(RTG_LEVEL_SUBSTATE, "    rtg_unlock_thread: "
 				"cpu=%d comm=%s sched=%s pid=%d tgid=%d rtgid=%d\n", cpu,
 				thread->comm, PRINT_SCHED(thread),
-				thread->pid, thread->tgid, thread->rtgid);
+				thread->pid, thread->tgid, GET_RTG_INFO(thread)->gid);
 		}
 	}
 
-	if (cpumask_weight(rtg_lock->locked_cores) == 0) {
+	/*
+	 * It is possible for one of the threads of the 'prev' task to be
+	 * holding the NPP lock. In that case, there is no point in sending
+	 * the rescheduling IPI to the blocked cores.
+	 *
+	 * We still want to allow the 'prev' thread to release the RT-Gang lock
+	 * on this core and go out of schedule if it is done; we don't want to
+	 * schedule the blocked cores just yet.
+	 */
+	if ((rtg_lock->no_preempt == 0) &&
+			(cpumask_weight(rtg_lock->locked_cores) == 0)) {
 		/* RT-Gang lock is now free. Reschedule blocked cores */
 		rtg_lock->leader = NULL;
 		rtg_lock->busy = false;
@@ -170,7 +184,7 @@ void rtg_try_release_lock(struct task_struct *prev)
  * Check if the next task is eligibile to obtain RT-Gang lock. If not, block
  * the task from executing on this CPU.
  */
-int rtg_try_acquire_lock(struct task_struct *next)
+int rtg_try_acquire_lock(struct task_struct *next, struct task_struct *prev)
 {
 	int this_cpu = smp_processor_id();
 	int ret = RTG_CONTINUE;
@@ -214,11 +228,38 @@ int rtg_try_acquire_lock(struct task_struct *next)
 		((IS_EDF(next) && IS_EARLIER_EDF(next, rtg_lock->leader)) ||
 		(!IS_EDF(next) && IS_HIGHER_PRIO(next, rtg_lock->leader)))) ||
 	   ((!IS_SAME_CLASS(next, rtg_lock->leader)) && IS_EDF(next))) {
+		/*
+		 * This is technically a valid preemption attempt but we may
+		 * not want to grant it if the running task currently holds
+		 * the NPP lock. In this case, record that HP task is waiting
+		 * and disallow preemption at the moment.
+		 */
+		if (rtg_lock->no_preempt > 0) {
+			rtg_lock->hp_waiting = true;
+			rtg_log_event(RTG_LEVEL_STATE, next, "npp-block");
+			cpumask_set_cpu(this_cpu, rtg_lock->blocked_cores);
+
+			/*
+			 * Reacquire the RT-Gang lock on behalf of the 'prev'
+			 * thread since it cannot be preempted at the moment.
+			 */
+			gang_lock_cpu(prev);
+
+			ret = RTG_NPP_BLOCK;
+			goto out;
+		}
+
 		rtg_log_event(RTG_LEVEL_STATE, next, "preemptor");
 		rtg_log_event(RTG_LEVEL_STATE, rtg_lock->leader, "preemptee");
 
 		do_gang_preemption(next);
 
+		/*
+		 * The 'prev' task will now be blocked on this cpu. We must
+		 * update the blocked core mask to reflect that.
+		 */
+		cpumask_set_cpu(this_cpu, rtg_lock->blocked_cores);
+
 #ifdef CONFIG_SCHED_THROTTLE
 		th_rtg_update_budget(GET_RTG_INFO(next)->rd_th,
 					GET_RTG_INFO(next)->wr_th);
@@ -238,6 +279,70 @@ out:
 	return ret;
 }
 
+SYSCALL_DEFINE1(npplock, int, lock)
+{
+	int ret = 0;
+
+	if (lock != 0 && lock != 1) {
+		printk(KERN_WARNING "[RT-GANG] NPP lock must be 0 or 1!");
+		return -EINVAL;
+	}
+
+	if (!(IS_RTC(current) || IS_EDF(current))) {
+		printk(KERN_WARNING "[RT-GANG] Requesting NPP lock "
+				"from non-rt thread!");
+		return -EACCES;
+	}
+
+	if (!(IS_GANG_MEMBER(current))) {
+		printk(KERN_ERR "[RT-GANG] Requesting NPP lock "
+				"but task does not have RT-Gang lock!");
+		return -EINVAL;
+	}
+
+	raw_spin_lock(&rtg_lock->access_lock);
+
+	if (lock == 0) {
+		if (rtg_lock->no_preempt == 0) {
+			printk(KERN_ERR "[RT-GANG] Trying to release a "
+					"free lock!");
+
+			ret = -EPERM;
+			goto spin_out;
+		}
+
+		rtg_log_event(RTG_LEVEL_STATE, current, "npp-release");
+		rtg_lock->no_preempt--;
+
+		/*
+		 * Task has come out of non-preemptive critical section and an
+		 * HP task is waiting. Reschedule CPUs.
+		 */
+		if (rtg_lock->no_preempt == 0 && rtg_lock->hp_waiting) {
+			rtg_log_event(RTG_LEVEL_SUBSTATE, current,
+					"    npp-free");
+			rtg_lock->hp_waiting = false;
+			resched_cpus(rtg_lock->blocked_cores);
+			cpumask_clear(rtg_lock->blocked_cores);
+		}
+	} else {
+		if (rtg_lock->hp_waiting) {
+			ret = -EBUSY;
+			goto spin_out;
+		}
+
+		rtg_log_event(RTG_LEVEL_STATE, current,
+				"npp-lock");
+		rtg_lock->no_preempt++;
+	}
+
+spin_out:
+	raw_spin_unlock(&rtg_lock->access_lock);
+
+
+	return ret;
+}
+
 /*
  * rtg_init_lock - Initialize RT-Gang data-structure and interface
  *
@@ -257,14 +362,16 @@ static int __init rtg_init_lock(void)
 	if (!debugfs_create_u32("debug_level", mode, dir, &rtg_debug_level))
 		goto fail;
 
-	raw_spin_lock_init(&rtg_lock->access_lock);
 	rtg_lock->busy = false;
+	rtg_lock->leader = NULL;
+	rtg_lock->no_preempt = 0;
+	rtg_lock->hp_waiting = false;
+	raw_spin_lock_init(&rtg_lock->access_lock);
 	zalloc_cpumask_var(&rtg_lock->locked_cores, GFP_KERNEL);
 	zalloc_cpumask_var(&rtg_lock->blocked_cores, GFP_KERNEL);
-	rtg_lock->leader = NULL;
 
 	for (; i < NR_CPUS; i++)
-		rtg_lock->gthreads [i] = NULL;
+		rtg_lock->gthreads[i] = NULL;
 
 	return 0;
 fail:
diff --git a/kernel/sched/rtgang.h b/kernel/sched/rtgang.h
index 5ce1445..99310c3 100644
--- a/kernel/sched/rtgang.h
+++ b/kernel/sched/rtgang.h
@@ -6,6 +6,7 @@
 #define RTG_FIFO_PRIO_THRESHOLD		(50)
 #define	RTG_CONTINUE			(0)
 #define RTG_BLOCK			(1)
+#define RTG_NPP_BLOCK			(2)
 
 #define RTG_FIFO_CHECK(p)					\
 	(p->mm && p->prio > RTG_FIFO_PRIO_THRESHOLD)
@@ -18,7 +19,7 @@
 		(GET_RTG_INFO(rtg_lock->leader)->gid == GET_RTG_INFO(p)->gid))
 
 #define IS_GANG_MEMBER(p)					\
-	(IS_REAL_GANG_MEMBER(p) || IS_VIRT_GANG_MEMBER(p))
+	(rtg_lock->busy && (IS_REAL_GANG_MEMBER(p) || IS_VIRT_GANG_MEMBER(p)))
 
 #define IS_SAME_CLASS(p, n)					\
 	(p->sched_class == n->sched_class)
@@ -41,14 +42,14 @@
 #define PRINT_PRIO(p)						\
 	(IS_EDF(p)? p->dl.deadline:(u64)p->prio)
 
-#undef RTG_DEBUG
+#define RTG_DEBUG 1
 #ifdef RTG_DEBUG
 #define rtg_log_event(level, task, event)			\
 do {								\
 	rtg_debug(level, "rtg_%s: comm=%s rtgid=%d tgid=%d "    \
 			"pid=%d prio=%d\n", event, task->comm, 	\
-			task->rtgid, task->tgid, task->pid,	\
-			task->prio);				\
+			GET_RTG_INFO(task)->gid, task->tgid,	\
+			task->pid, task->prio);			\
 } while (0);
 
 #define rtg_debug(level, format, ...)				\
@@ -69,6 +70,8 @@ do {								\
 
 struct rtgang_lock {
 	bool			busy;
+	bool			hp_waiting;
+	int			no_preempt;
 	raw_spinlock_t		access_lock;
 	struct task_struct*	leader;
 	struct task_struct*	gthreads [NR_CPUS];
@@ -77,7 +80,7 @@ struct rtgang_lock {
 };
 
 void rtg_try_release_lock(struct task_struct *prev);
-int rtg_try_acquire_lock(struct task_struct *next);
+int rtg_try_acquire_lock(struct task_struct *next, struct task_struct *prev);
 
 #endif /* CONFIG_SCHED_RTGANG */
 
